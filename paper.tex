\documentclass[a4paper,12pt]{report}

\usepackage{alltt, fancyvrb, url}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{wrapfig}
\usepackage{algorithmic}
\usepackage[utf8]{inputenc}
\usepackage{fontenc}
\usepackage{amsmath,stmaryrd,mathtools,algorithm}
\usepackage{amssymb}
%\usepackage{amsfonts}
\usepackage{float}
\usepackage{hyperref}
\usepackage{listings}
\usepackage{url}

\usepackage[english]{cleveref}

\usepackage[english]{babel}

\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{property}{Property}[section]
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\dd}{\cdot}

\title{Introduction to LLL\\``Cryptography''}
 
\author{Di Santi Giovanni}
\date{\today}

\begin{document}
 
\maketitle

\tableofcontents

\chapter{Linear Algebra Background}

\section{Vector Spaces}

\begin{definition}
    \textbf{Vector space}.
\end{definition}
A \texttt{vector space} $V$ is a subset of $\R^{m}$ which is closed under finite vector addition and scalar multiplication, with the property that

\begin{center}
   $a_1v_1 + a_2v_2 \in V$ for all $v_1,v_2 \in V$ and all $a_1,a_2 \in \R$
\end{center}

\begin{definition}
    \textbf{Linear Combinations}
\end{definition}

Let $v_1,v_2,\ldots,v_k \in V$. A \texttt{linear combination} of $v_1,v_2,\ldots,v_k \in V$ is any vector of the form

\begin{center}
    $\alpha_1v_1 + \alpha_2v_2 + \cdots + \alpha_kv_k$ with $\alpha_1, \ldots, \alpha_k \in \R$
\end{center}

\begin{definition}
    \textbf{Lineaer Independece}
\end{definition}

A set of vectors $v_1,v_2,\ldots,v_k \in V$ is \texttt{linearly independent} if the the only way to get

\begin{center}
    $a_1v_1 + a_2v_2 + \cdots + a_kv_k = 0$
\end{center}

is to have $a_1 = a_2 = \cdots = a_k = 0$.

\begin{definition}
    \textbf{Bases}
\end{definition}

Taken a set of linearly independent vectors $b = (v_1,\ldots,v_n) \in V$ we say that $b$ is a \texttt{basis} of $V$ if $\forall w \in V$ we can write

\begin{center}
    $w = a_1v_1 + a_2v_2 + \cdots + a_nv_n$
\end{center}

\begin{definition}
    \textbf{Vector's length}
\end{definition}

The vector's length or \texttt{Euclidean norm} of $v = (x_1, x_2, \ldots, x_m)$ is

\begin{center}
    $\lVert v \rVert = \sqrt{x_1^2 + x_2^2 + \cdots + x_m^2}$
\end{center}

\begin{definition}
    \textbf{Dot Product}
\end{definition}

Let $v, w \in V \subset \R^m$ and $v = (x_1, x_2, \ldots, x_m), w = (y_1, y_2, \ldots, y_m)$, the \texttt{dot product} of $v$ and $m$ is

\begin{center}
    $v \dd m = x_1y_1 + x_2y_2 + \cdots + x_my_m$

    or

    $v \dd m = \lVert v \rVert \lVert w \rVert \cos{\theta}$
\end{center}

where $\theta$ is the angle between $v$ and $w$ if we place the starting points of the vectors at the origin $O$.

\begin{figure}[!b]
    \centering
    \includegraphics[scale=0.2]{./img/dot_product.png}
    \caption{Dot Product By 3Blue1Brown}
    \label{fig:dot_product}
\end{figure}

Geometrically speaking $v \cdot m$ is the length of $w$ projected to $v$ multiplied by the length of $v$ as shown in \ref{fig:dot_product}

\begin{definition}
    \textbf{Ortoghonal Basis} 
\end{definition}

An \texttt{ortoghonal basis} for a vector space $V$ is a basis $v_1, \ldots, v_m$ with the property that

\begin{center}
    $v_i \dd v_j = 0$ for all $i \neq j$
\end{center}

If $\lVert v_i \rVert = 1$ for all $i$ then the basis is \texttt{orthonormal}.

\begin{algorithm}
    \textbf{Gram-Schmidt Algorithm}
\end{algorithm}

Let $b = (v_1, \ldots, v_n)$, be a basis for a vector space $V \subset \R^m$. There is an algorithm to create an orthogonal basis
$b^* = (v_1^*,\ldots,v_n^*)$.
The two bases have the property that Span$\{v_1,\ldots,v_i\}$ = Span$\{v_1^*,\ldots,v_i^*\}$ for all $i = 1,2,\ldots,n$

\begin{figure}[htpb]
    \centering
    \includegraphics[width=0.5\textwidth]{./img/gram_schmidt.png}
    \caption{Gram Schmidt orthogonalization}
    \label{fig:gram_schmidt}
\end{figure}

If we take $v_1=(4, 1), v_2=(2, 3)$ as basis and apply gram schmidt we obtain $u_1=v_1=(4, 1), u_2=(-10/17, 40/17)$ as shown in \ref{fig:gram_schmidt}

\section{Lattices}

\begin{definition}
    \textbf{Lattice}
\end{definition}

Let $v_1,\ldots,v_n \in \R^m, m \ge n$ be linearly independent vectors. A \texttt{Lattice} $L$ spanned by $\{v_1,\ldots,n_n\}$ is the set of 
all integer linear combinations of $v_1,\ldots,v_n$.

\begin{center}
    $L = \bigg\{\sum_{i=1}^{n} a_iv_i, a_i \in \Z \bigg\}$
\end{center}

If $v_i$ for every $i = 1,\ldots\,n$ has integer coordinates then the lattice is
called \texttt{Integral Lattice}.

On the figure \ref{fig:lattice0} we show a lattice $L$ with bases $v=(3, 1)$ and $w=(-1, 1)$, and on \ref{fig:lattice1} the same lattice $L$ with
a different basis.

\begin{figure}[!tbp]
    \begin{minipage}[b]{0.50\textwidth}
        \includegraphics[width=\textwidth]{./img/lattice_b0.png}
        \caption{Lattice $L$ spanned by $v, w$}
        \label{fig:lattice0}
    \end{minipage}
    \hspace{\fill}
    \hspace{\fill}
    \hspace{\fill}
    \hspace{\fill}
    \hspace{\fill}
    \begin{minipage}[b]{0.50\textwidth}
        \includegraphics[width=\textwidth]{./img/lattice_b1.png}
        \caption{Lattice $L$ spanned by $v', w'$}
        \label{fig:lattice1}
    \end{minipage}
\end{figure}

\section{Problems}

\subsection{SVP}

\textbf{The Shortest Vector Problem} (\texttt{SVP}): Find a nonzero vector $v \in L$ that minimez the Euclidean norm $\lVert v \rVert$.

\begin{algorithm}
    \textbf{Gauss Reduction}
\end{algorithm}

\begin{figure}[b]
    \centering
    \includegraphics[width=0.5\textwidth]{./img/gauss_svp.png}
    \caption{Gauss reduction}
    \label{fig:gauss_svp}
\end{figure}

Gauss's developed an algorithm to find an optimal basis for a two-dimensional lattice given an arbitrary basis. The output of the algorithm
gives the shortest nonzero vector in $L$ and in this way solves the \texttt{SVP}.

If we take for example $v_1 = (8, 4), v_2 = (7, 5)$ and apply the gauss reduction algorithm we
obtain $w_1 = (1, -1), w_2 = (6, 6)$ \ref{fig:gauss_svp}. $w_1$ is the shortest nonzero vector in the lattice $L$ spanned by $v_1, v_2$.

However the bigger the dimension of the lattice, the harder is the problem and there isn't a polynomial algorithm to find such vector.

\subsection{CVP}

\textbf{The Closest Vector Problem} (\texttt{CVP}): Given a vector $w \in \R^m$ that is not in $L$, find a vector $v \in L$ that is closest to $w$,
in other words find a vector $v \in L$ that minimizes the Euclidean norm $\lVert w - v \rVert$.

TODO FIX \ref{fig:cvp} is wrong, the lattice is different.

\begin{figure}[t]
    \centering
    \includegraphics[width=0.5\textwidth]{./img/cvp.png}
    \caption{CVP}
    \label{fig:cvp}
\end{figure}

TODO: CVP and SVP are related.

% matrix([[0, 0, 1], [1, 1, 2], [1, 2, 1]])
\chapter{LLL}

\section{Introduction}

The \textbf{Lenstra-Lenstra-Lov√°sz} \textit{LLL} or \textit{$L^3$} is a polynomial time algorithm to find a "shorter" basis.

\begin{theorem}
    \textbf{LLL}
\end{theorem}

\textit{Let $L \in \Z^n$ be a lattice spanned by $B = \{v_1,\ldots,v_n\}$. The LLL algorithm outputs a reduced lattice
basis $\{w_1, \ldots, w_n\}$ with}

\begin{center}
    $\lVert w_i \rVert \le 2^{\frac{n(n-1)}{4(n-i+1)}} det(L)^{\frac{1}{n-i+1}}$ for $i=1,\ldots,n$
\end{center}

\textit{in time polynomial in n and in the bit-size of the entries of the basis matrix $B$}.\\

Basically the first vector of the new basis will be as short as possible, and the other will have increasing lengths. The new vectors
will be as orthogonal as possible to one another, i.e., the dot product $w_i \dd w_j$ will be close to zero.

\subsection*{Example}

For example we can take the following basis (the rows are the vector) that span
a lattice $L$.

\[
L = 
\begin{pmatrix}
    4 & 9 & 10\\
    2 & 1 & 30\\ 
    3 & 7 & 9
\end{pmatrix}
\] 

Applying the LLL algorithm we obtain

\[
LLL(L) = 
\begin{pmatrix}
    -1 & -2 & -1\\
     3 & -2 &  1\\
    -1 & -1 &  5
\end{pmatrix}
\] 

Where the first row is the shortest vector in the lattice $L$, and so solves the \textbf{SVP} problem.
For higher dimensions however the LLL algorithm outputs only an approximation for the \textbf{SVP} problem.

\section{Algorithm}

TODO: Write algorithm and explain some steps

\section{Applications}

There are many applications of LLL

\begin{enumerate}
    \item Factoring polynomials over the integers. For example, given $x^2 - 1$ factor it into $x + 1$ and $x - 1$.
    \item Integer Programming. This is a well-known \textbf{NP}-complete problem. Using LLL, one can obtain a polynomial time solution
          to integer programming with a fixed number of variables.
    \item Approximation to the \textbf{CVP} or \textbf{SVP}, as well as other lattice problems.
    \item Application in cryptanalysis.
\end{enumerate}

\chapter{Cryptanlysis}

% \section{Attack Knapsack}

\section{RSA Introduction}

\subsection{Algorithm}

\textbf{RSA} is one of the earliest and most used asymmetric cryptosystem.
The usual step to generate a public/private key for \textbf{RSA} is the following

\begin{enumerate}
    \item Fix $e = 65537$ or $e = 3$ (public).
    \item Find two primes $p, q$ such that $p - 1$ and $q - 1$ are relatively prime to $e$, i.e. gcd$(e, p-1) = 1$ and gcd$(e, q-1) = 1$.
    \item Compute $N = p * q$ and $\phi(n) = (p-1) * (q-1)$
    \item Calculate $d$ (private) as the multiplicative inverse of $e$ modulo $\phi(n)$.
    \item $(N, e)$ is the public key, $(N, d)$ is the private key.
\end{enumerate}

To encrypt a message $m$ with \textbf{textbook RSA}

\begin{center}
    $c = m^e \mod N$
\end{center}

To decrypt a ciphertext $c$

\begin{center}
    $m = c^d \mod N$
\end{center}

\subsection{Security}

RSA relies on the hardness of factoring the modulo $N$ and we don't have a polynomial algorithm to factor it, so it's considered secure.
However there are different attacks against textbook RSA or a relaxed model of RSA:

\begin{enumerate}
    \item If $m < N^{\frac{1}{e}}$, then $m^e < N$ and the modulo operation is not applied and we only need to find the $e$th root of $c$ over the
        integers to find $m$.
    \item If $q = p + x$ where $x$ is small enough than it's easy to recover the factors of $N$ computing $A = \sqrt{N}$ and compute
            $p = A - x$ for different value of $x$ until $N \mod p = 0$, then we have found $q = N / p$
    \item Many more attacks can be found TODO.
\end{enumerate}

\section{Lattices against RSA}

We want to attack a relaxed model of RSA where we know a part of the message $m$.
We start introducing the \ref{TODO} coppersmith attack and the math behind it.

\subsection{Mathematical introduction}

It's easy to find the roots of a univariate polynomial over the integers. Finding the roots of \textbf{modular} polynomial is hard, example:

\begin{center}
    $f(x) \equiv 0 \mod N$
\end{center}

Suppose $N$ is an \textbf{RSA} modulus and we don't know the factorization of it. Let's have an univariate integer polynomial $f(x)$ with degree $n$

\begin{center}
    $f(x) = x^n + a_{n-1}x^{n-1} + a_{n-2}x^{n-2} + \cdots + a_1n + a_0$
\end{center}

Coppersmith showed how we can recover the value $x_0$ such that $f(x_0) \equiv 0 \mod N$, with $x_0 < N^{\frac{1}{n}}$ in polynomial time
using the following theorem

\begin{theorem}
    \textbf{Howgrave-Graham}
\end{theorem}

\textit{Let $g(x)$ be an univariate polynomial with n monomials and $m$ be a positive integer.
    If we have some restraint $X$ and the following equations hold}

\begin{center}
    \begin{eqnarray}
        g(x_0) \equiv 0 \mod N^m, |x_0| \le X \label{eq:bound} \\
         \lVert g(xX) \rVert < \frac{N^m}{\sqrt{n}} \label{eq:pol}
    \end{eqnarray}
\end{center}

\textit{Then $g(x_0) = 0$ holds over the integers.}

\vspace*{10px}

This theorem states that is possible to compute the root of $f(x) \mod N$ if we can find a polynomial $g(x)$ that share the same root but modulo $N^m$.
If \ref{eq:bound} and \ref{eq:pol} hold then we can simply compute the root of $g(x)$ over the integers to have the same root $x_0$
such that $f(x_0) \equiv 0 \mod N$.

Howgrave-Graham's idea is to find this polynomial $g$ by combining polynomials $p_i$ who also have $x_0$ as roots modulo $N^m$.

\vspace*{10px}

The \textbf{LLL} algorithm is fundamental because:

\begin{itemize}
    \item It only does \textbf{integer linear operations} on the basis vectors.
        In this way even if the basis is different it's only a linear combination of vector that still have $x_0$ as root
        modulo $N^m$.
    \item If we craft the lattice properly, the norm of shortest vector on the reduced basis will satisfy \ref{eq:pol}.
        We know the length's bound of the shortest vector that LLL could find.
\end{itemize}

We can easily create polynomials $p_i$ ($g_{i,j}$ and $h_i$)  sharing the same root $x_0$ over $N^m$ of $f$ where $\delta$ is the degree of $f$:

\begin{center}
    \begin{eqnarray}
        g_{i,j}(x) &= x^j \cdot N^i \cdot f^{m-i}(x) \text{ for } i = 0,\hdots,m-1,\hspace{2mm} j=0,\hdots,\delta-1 \label{eq:create_pol} \\
        h_i(x) &= x^i \cdot f^m(x) \text{ for } i = 0,\hdots,t-1 
    \end{eqnarray}
\end{center}

Applying \textbf{LLL} to a lattice constructed by the coefficients of \ref{eq:create_pol} we'll find a short vector $v = g(xX)$ that will
satisfy \ref{eq:pol}. If we then take $g(x)$ we will be able to compute the root over the integer.

\subsection{Example}

We have a 100-bit \textbf{RSA} modulus

\begin{center}
    $N=0xf046522fb555a90bdc558fc93$ and $e = 3$.
\end{center}

Before the encryption the message $m$ is padded as

\[
    z = pad || m = 0x74686973206b65793a || m
\]

where $||$ is the concatenation. The padding is the ascii encoding of ``this key:''\\

The ciphertext is

\[
    c = z^e \mod N = 0x5b603cda4b72100c6f25954fc
\]

Suppose that we don't know the factorization of $N$ and we would like to know the message $m$.
However we know the padding and that the length of $m < 2^{16}$.

\vspace*{10px}

Let's define

\[
a = 0x74686973206b65793a0000
.\] 

which is the known padding string that got encrypted.

\vspace*{10px}

Thus we have that $c = (a + m)^3 \mod N$, for an unkown small $m$.
We can define $f(x) = (a + x)^3 - c$, and so we setup the problem to find a small root $m$ such that $f(m) \equiv 0 \mod N$

\begin{align*}
    f(x) = x^3 + 0x15d393c596142306bae0000x^2 + 0x1b53c5e184a49b39f9ad9eedbx\\
    + 0x486a5d936fb568185c8ff0506
\end{align*}

\vspace*{10px}

\textbf{Lattice contruction}. Let the coefficients of $f$ be $f(x) = x^3 + f_2x^2 + f_1x + f_0$
and $X = 2^{16}$ be the upper bound of the size of the root $m$. We can construct the matrix

\[
B = 
\begin{pmatrix}
    X^3 & f_2X^2 & f_1X & f_0 \\
    0 & NX^2 & 0 & 0 \\
    0 & 0 & NX & 0 \\
    0 & 0 & 0 & N \\
\end{pmatrix}
\] 

The rows of the matrix correspond to the coefficient vectors of the polynomials $f(x), Nx^2, Nx$ and $N$, furthermore we know
that each polynomials will be 0 modulo $N$ if evaluated at $x = m$.
We applied Howgrave-Graham with $m=1$ (the $N^m$ parameter not the message).\\
With this lattice construction every vector is of the form $v = (v_1X^3, v_2X^2, v_1X, v_0)$, because any
integer linear combination of the vector of the lattice will keep the bound $X^i$ for $i=0,..,\dim(B)-1$.

\textbf{Apply LLL}. We then apply LLL to find the shortest vector of the reduced basis:

\[
    \begin{split}
    v = (0x90843131bc53X^3 + 0x2736f60b1c7ba3294X^2, \\
    -0x1bec331b20625341b6d73X, 0x47336b98335c143ac912ec9e)
    \end{split}
\]

We can construct the polynomial $g$ using the coefficients of $v$

\[
    \begin{split}
        g(x) = 0x90843131bc53x^3 + 0x2736f60b1c7ba3294x^2 \\
        -0x1bec331b20625341b6d73x + 0x47336b98335c143ac912ec9e
    \end{split}
\]

We know that

\[
    g(x_0) \equiv 0 \mod N, |x_0| \le X
\]

What we need to prove is that

\[
    \lVert g(xX) \rVert \le \frac{N}{\sqrt{n}}
\] 

In this example, det $B = X^6N^3$, and LLL will find a short vector with $\lVert v \rVert \le 2^{\frac{n(n-1)}{4(n)}} (\det B)^{\frac{1}{n}}$.
If we ignore the $2^{\frac{3}{4}}$ factor (remember that $n = 4$), then we need to satisfy

\[
    g(m) \le \lVert v \rVert \le (\det B)^{\frac{1}{4}} < \frac{N}{\sqrt{4}}
\] 

\vspace*{10px}

We have $(\det B)^{\frac{1}{4}} = (X^6N^3)^{\frac{1}{4}} < \frac{N}{\sqrt{4}}$, if we solves for $X$ this will be satisfied when $X < (\frac{N}{16})^{\frac{1}{6}}$.
With the numbers we have this inequality is verified, however even if the bound of the shortest vector is larger we still have some possibilities to
find the correct root.

If we compute the root of $g(x)$ over the integers we obtain $m = 0x6162$ which is the correct result.\\

This specific lattice works to find roots up to size $N^{\frac{1}{6}}$, so the same construction will work if we want to find

\begin{itemize}
    \item \textasciitilde 170 unkown bits of message from an RSA 1024-bit modulus
    \item \textasciitilde 341 unkown bits of message from an RSA 2048-bit modulus
    \item \textasciitilde 683 unkown bits of message from an RSA 4096-bit modulus
\end{itemize}

To compute bigger root a bigger lattice with more polynomials generated with \ref{eq:create_pol} is needed,
this method is better described in TODO, but the principles are the same.

\section{ECDSA Introduction}

\subsection{Algorithm}

\textbf{ECDSA} is a variant of the Digital Signature Algorithm (\textbf{DSA}) which uses elliptic curve cryptography.
To digitally sign a message we have 3 public parameters

\begin{itemize}
    \item The elliptic curve \textbf{$E$}.
    \item The generator point \textbf{$G$}.
    \item The generator's order \textbf{$n$}.
\end{itemize}

We also need to create a private key $d \in [1, n-1]$ and public key $Q = dG$. To digitally \textbf{sign} a message $m$:

\begin{enumerate}
    \item Compute $h = $ HASH$(m)$ where HASH is a cryptographic hash functions.
    \item Select a random integer $k \in [1, n-1]$.
    \item Calculate $P = kG = (x_1, y_1)$ and set $r = (x_1)$.
    \item Compute $s = k^{-1}(h + dr) \mod n$.
\end{enumerate}

\vspace*{10px}

To \textbf{verify} the signature:

\begin{enumerate}
    \item Compute $h = $ HASH$(m)$.
    \item Calculate $u_1 = hs^{-1} \mod n$ and $u_2 = rs^{-1} \mod n$.
    \item Compute $P = u_1G + u_2Q = (x_1, y_1)$.
    \item If $r \equiv x_1 \mod n$ then the signature is valid.
\end{enumerate}

\subsection{Security}

\section{Lattices against ECDSA}

\subsection{Example}

\chapter*{End of Paper}

$gg^2$

\bibliographystyle{plain}
\bibliography{paper}

\end{document}

